"""
Common utilities for contextual training and evaluation.
"""

import os
import yaml
import torch
from typing import List, Dict, Any, Optional

from .context_attention import generate_with_contextual_mask

try:
    import wandb
    HAS_WANDB = True
except ImportError:
    HAS_WANDB = False

# Global list for accumulating eval results across steps
# We store data as a list and create a fresh Table each time to avoid immutability issues
_eval_table_data: Optional[List[List]] = None


def format_conversation(data: List[str], tokenizer=None) -> List[Dict[str, str]]:
    """
    Convert ultrachat data format to chat messages.
    
    Args:
        data: list of alternating [user, assistant, user, assistant, ...]
        tokenizer: Optional tokenizer (unused, kept for compatibility)
        
    Returns:
        List of message dicts with 'role' and 'content'
    """
    messages = []
    for i, text in enumerate(data):
        role = "user" if i % 2 == 0 else "assistant"
        messages.append({"role": role, "content": text})
    return messages


def load_eval_examples(yaml_path: str) -> List[Dict[str, Any]]:
    """
    Load evaluation examples from YAML file.
    
    Args:
        yaml_path: Path to YAML file
        
    Returns:
        List of example dicts with 'name', 'messages', 'drop_rounds'
    """
    with open(yaml_path, "r") as f:
        data = yaml.safe_load(f)
    return data.get("examples", [])


@torch.no_grad()
def run_evaluation(
    model,
    tokenizer,
    eval_examples: List[Dict[str, Any]],
    accelerator,
    global_step: int,
    max_new_tokens: int = 256,
    use_drop: bool = False,
) -> List[Dict[str, Any]]:
    """
    Generate responses for evaluation examples with sequential multi-round generation.
    Each round's response is generated by the model and used as context for the next round.
    
    Args:
        model: The model to evaluate
        tokenizer: Tokenizer for the model
        eval_examples: List of evaluation examples from YAML
        accelerator: Accelerator instance
        global_step: Current training step
        max_new_tokens: Max tokens to generate per response
        use_drop: If True, apply drop_rounds using contextual attention mask
        
    Returns:
        List of evaluation results
    """
    model.eval()
    unwrapped_model = accelerator.unwrap_model(model)
    
    eval_results = []
    
    for example in eval_examples:
        name = example.get("name", "unnamed")
        original_messages = example.get("messages", [])
        drop_rounds = example.get("drop_rounds", [])
        
        if not original_messages:
            continue
        
        # Extract user messages from the example
        user_messages = [m["content"] for m in original_messages if m["role"] == "user"]
        
        # Generate responses sequentially, using model's own responses as context
        messages = []
        generated_responses = []
        
        for turn_idx, user_msg in enumerate(user_messages):
            # Add user message
            messages.append({"role": "user", "content": user_msg})
            
            # Determine which rounds to drop for THIS turn
            # drop_rounds can be:
            #   - A dict: {round_where_drop_happens: [rounds_to_drop]}
            #   - A list: [rounds_to_drop] (legacy, drop at last round)
            current_round = turn_idx + 1  # Round numbering: 1-indexed for generation
            
            if use_drop and drop_rounds:
                if isinstance(drop_rounds, dict):
                    # New format: get rounds to drop at this specific round
                    # Accumulate all rounds that should be dropped by now
                    rounds_dropped_by_now = []
                    for drop_at, rounds in drop_rounds.items():
                        if drop_at <= current_round:
                            rounds_dropped_by_now.extend(rounds)
                    effective_drop_rounds = rounds_dropped_by_now
                else:
                    # Legacy list format: drop all specified rounds
                    effective_drop_rounds = drop_rounds
            else:
                effective_drop_rounds = []
            
            response = generate_with_contextual_mask(
                unwrapped_model, tokenizer, messages, effective_drop_rounds, max_new_tokens
            )
            
            # Add model's response to conversation context
            messages.append({"role": "assistant", "content": response})
            generated_responses.append(response)
        
        eval_results.append({
            "name": name,
            "user_messages": user_messages,
            "generated_responses": generated_responses,
            "drop_rounds": drop_rounds,
            "full_conversation": messages,
        })
    
    # Log results
    if accelerator.is_main_process and eval_results:
        _print_eval_results(eval_results, global_step, use_drop)
        if HAS_WANDB:
            try:
                _log_to_wandb(accelerator, eval_results, global_step)
            except Exception as e:
                print(f"Failed to log to wandb: {e}")
    
    model.train()
    return eval_results


def _print_eval_results(eval_results: List[Dict[str, Any]], global_step: int, use_drop: bool = False):
    """Print evaluation results to console."""
    print(f"\n{'='*60}")
    mode = "with drop" if use_drop else "no drop"
    print(f"ðŸ“‹ Evaluation at step {global_step} ({mode})")
    print(f"{'='*60}")
    
    for r in eval_results:
        print(f"\n[{r['name']}] (drop_rounds={r['drop_rounds']})")
        for turn_idx, (user, response) in enumerate(zip(r['user_messages'], r['generated_responses'])):
            print(f"  Turn {turn_idx + 1}:")
            print(f"    Q: {user[:80]}{'...' if len(user) > 80 else ''}")
            print(f"    A: {response[:150]}{'...' if len(response) > 150 else ''}")
    
    print(f"{'='*60}\n")


def _log_to_wandb(accelerator, eval_results: List[Dict[str, Any]], global_step: int):
    """Log evaluation results to wandb.
    
    Accumulates data in a list and creates a new table each time to avoid
    the 'mutating immutable table' warning.
    """
    global _eval_table_data
    
    # Initialize data list on first call
    if "_eval_table_data" not in globals() or _eval_table_data is None:
        globals()["_eval_table_data"] = []
    
    # Append new rows to the data list
    for r in eval_results:
        for turn_idx, (user, response) in enumerate(zip(r['user_messages'], r['generated_responses'])):
            _eval_table_data.append([
                global_step,
                r["name"],
                turn_idx + 1,
                user,
                response,
                str(r["drop_rounds"]),
            ])
    
    # Create a fresh table with all accumulated data
    table = wandb.Table(
        columns=["step", "name", "turn", "question", "response", "drop_rounds"],
        data=_eval_table_data,
    )
    
    # Log the table
    wandb_tracker = accelerator.get_tracker("wandb")
    if wandb_tracker:
        wandb_tracker.log({"eval/responses": table}, step=global_step)
    else:
        accelerator.log({"eval/responses": table}, step=global_step)


def reset_eval_table():
    """Reset the evaluation table data. Call at the start of a new training run."""
    global _eval_table_data
    _eval_table_data = None

